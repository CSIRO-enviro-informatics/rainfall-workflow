{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Guide\n",
    "\n",
    "This guide will go through the main functions of the rainfall workflow to show how it can be executed.\n",
    "\n",
    "The main file of this project is \"workflow.py\". When it is run, these functions execute:\n",
    "\n",
    "    daily_jobs()\n",
    "    check_for_bad_smips()\n",
    "    create_parameter_files()\n",
    "    create_forecast_files(date)\n",
    "    create_shuffled_forecasts()\n",
    "    \n",
    "As these functions cannot be run in their entirety for the sake of an example, their equivalents will be demonstrated here. While those functions would run through all (or bounded) data, consisting of thousands of sets of coordinates, this guide will use one set of coordinates.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Update data: daily_jobs()\n",
    "\n",
    "Before starting working with the data, update it. This function pulls ACCESS-G files from NCI (and restricts the data to Australia only), opens SMIPSv.05 and regrids the latest data to grids matching ACCESS-G, and saves both types of data as single files and to an aggregated file. The aggregated versions will be used in processing. \n",
    "\n",
    "Locations of all types of file are in settings.py. \n",
    "\n",
    "Running daily_jobs() on 18/11/2019, ACCESS-G data is collected until yesterday, and SMIPS data until the day before yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20191117']\n",
      "Connection succesfully established ... \n",
      "File: 2019/ACCESS_G_accum_prcp_fc_2019111712.nc written\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/and522/.conda/envs/rainfall-workflow/lib/python3.6/site-packages/iris/fileformats/_pyke_rules/compiled_krb/fc_rules_cf_fc.py:1932: UserWarning: Ignoring netCDF variable 'valid_time' invalid units 'hhmm UTC'\n",
      "  warnings.warn(msg)\n",
      "/home/and522/.conda/envs/rainfall-workflow/lib/python3.6/site-packages/iris/fileformats/_pyke_rules/compiled_krb/fc_rules_cf_fc.py:1932: UserWarning: Ignoring netCDF variable 'base_date' invalid units 'yyyymmdd'\n",
      "  warnings.warn(msg)\n",
      "/home/and522/.conda/envs/rainfall-workflow/lib/python3.6/site-packages/iris/fileformats/_pyke_rules/compiled_krb/fc_rules_cf_fc.py:1932: UserWarning: Ignoring netCDF variable 'base_time' invalid units 'hhmm UTC'\n",
      "  warnings.warn(msg)\n",
      "/home/and522/.conda/envs/rainfall-workflow/lib/python3.6/site-packages/iris/fileformats/_pyke_rules/compiled_krb/fc_rules_cf_fc.py:1932: UserWarning: Ignoring netCDF variable 'valid_date' invalid units 'yyyymmdd'\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20191116']\n",
      "/OSM/CBR/LW_SOILDATAREPO/work/SMIPSRegrid/2019/SMIPS_blnd_prcp_regrid_20191116.nc saved\n",
      "SMIPS aggregation is already up to date\n",
      "ACCESS-G aggregation is already up to date\n",
      "Daily jobs done\n"
     ]
    }
   ],
   "source": [
    "workflow.daily_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 check_for_bad_smips()\n",
    "A hopefully temporary function, this deals with errors found in several SMIPS dates. SMIPS rainfall is compared to a maximum threshold of 900 - if a date has values above this, that date's data is removed in SMIPS.nc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import settings\n",
    "\n",
    "coords = [-35.85938, 148.3594]\n",
    "lat = coords[0]\n",
    "lon = coords[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fit model: create_parameter_files()\n",
    "\n",
    "The first step in the workflow is to fit the bjpmodel on the data and save the parameters associated with each grid cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timezone found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/and522/PycharmProjects/rainfall-workflow/bjpmodel.py:66: RuntimeWarning: invalid value encountered in less_equal\n",
      "  censor_idx = fit_data <= censor\n",
      "/home/and522/PycharmProjects/rainfall-workflow/bjpmodel.py:70: RuntimeWarning: invalid value encountered in less\n",
      "  missing_idx = np.abs(fit_data - self.MISSING_DATA_VALUE) < 1E-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF Cube doesn't exist at  temp/params/grids/params_-35.85938_148.3594.nc\n",
      "time to generate parameters:  76.63444638252258  seconds\n"
     ]
    }
   ],
   "source": [
    "import parameter_cube\n",
    "\n",
    "fname = settings.params_filename(lat, lon)\n",
    "start = time.time()\n",
    "parameter_cube.generate_forecast_parameters(lat, lon)\n",
    "print('time to generate parameters: ', time.time()-start, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forecast: create_forecast_files(date)\n",
    "\n",
    "The next step is to read the saved parameters back into memory and create and save a forecast for your chosen (probably today's) date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timezone found\n",
      "NetCDF Cube doesn't exist at  temp/forecast/grids/forecast_20191117_-35.85938_148.3594.nc\n",
      "Timezone found\n",
      "Timezone found\n",
      "Timezone found\n",
      "Timezone found\n",
      "Timezone found\n",
      "Timezone found\n",
      "Timezone found\n",
      "Timezone found\n",
      "time to generate forecast:  1.478586196899414  seconds\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import transform\n",
    "import forecast_cube\n",
    "\n",
    "date = datetime.date(2019, 11, 17)\n",
    "\n",
    "start = time.time()\n",
    "mu, cov, tp = parameter_cube.read_parameters(lat, lon)\n",
    "fname = settings.forecast_filename(date, lat, lon)\n",
    "transform.transform_forecast(lat, lon, date, mu, cov, tp)\n",
    "print('time to generate forecast: ', time.time()-start, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Restore spatial correlations in forecast: create_shuffled_forecasts()\n",
    "The forecast is actually made up of 1000 ensemble members of forecasts. Next, use the Schaake shuffle to restore spatial correlations between grid points - get areas close to each other to have similar precipitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF Cube doesn't exist at  temp/forecast/shuffled/shuffledforecast_20191101_-35.859375_148.35938.nc\n",
      "time to shuffle forecast:  3.279794216156006  seconds\n"
     ]
    }
   ],
   "source": [
    "import source_cube\n",
    "\n",
    "start = time.time()\n",
    "date_sample = source_cube.sample_date_indices()\n",
    "lat_dict, lon_dict = source_cube.get_lat_lon_indices()\n",
    "lat_i = lat_dict[round(float(lat), 2)]\n",
    "lon_i = lon_dict[round(float(lon), 2)]\n",
    "transform.shuffle(lat_i, lon_i, date_sample)\n",
    "print('time to shuffle forecast: ', time.time()-start, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is all the work in the workflow at the moment. \n",
    "\n",
    "Next: hydrological model, soil moisture API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
